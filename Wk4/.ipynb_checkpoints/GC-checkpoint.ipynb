{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6882db",
   "metadata": {},
   "source": [
    "## About This Session\n",
    "\n",
    "This lab is a guided walkthrough to build a _basic_ 'image classification' DNN model and train it on some data.\n",
    "\n",
    "As we build up our model we will build a familiarity with the various components and strategies for building such an ANN system.\n",
    "\n",
    "After we have been able to do this we will then save and load our now-trained model from disk and use it to make a prediction on a new image which isn't present in the original dataset.\n",
    "\n",
    "\n",
    "_Responsible: Robert Currie (<rob.currie@ed.ac.uk>)_\n",
    "\n",
    "### Notes on assessment\n",
    "* Try and calculate the answers to the exercises provided. If you are unable to complete the question, describe which approach you _would_ have taken to solve the problem\n",
    "* Code must be understandable and reproducible. Before grading the notebook kernel **may** be restarted and re-run, so make sure that your code can run from start to finish without any (unintentional) errors\n",
    "* If you are unsure on how to proceed please **ask one of the TAs** during the workshop\n",
    " - Notebooks should be submitted by **10am on Friday 13th October**\n",
    " - This notebook starts with problem 0. This is not assessed, but it is a helpful exercise to show some of the potential pitfalls of 'trusting algorithms' to do the work for us.\n",
    "\n",
    "* There are 10 sections involved in building a DNN using TensorFlow, trainig it on data and then using it to predict based on a new piece of data never seen by the model before.\n",
    "* There are 2 bonus sections covering 'additional training of a model' and 'the impact of training on the distribution of internal weights within a DNN'. These have bonus points if you manage to finish them, but the final mark is capped at 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04643d-4607-4463-a842-91be11c11806",
   "metadata": {},
   "source": [
    "# Building an image classifier with a DNN using TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895060a6",
   "metadata": {},
   "source": [
    "##### Marking\n",
    "\n",
    "Marks for the different parts are shown below.\n",
    "\n",
    "* Questions are marked as starting with '**Q**' and can in principle be attempted at any time.\n",
    "* Problems are intended to be tackled in order, i.e. 1->9\n",
    "* If you jump in at problem 5 for instance you won't yet have a model to train without tackling the steps before.\n",
    "* There are bonus problems at the end to tackle but the maximum mark is 10/10\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 0. Dangers of Fitting                  | <p align='left'>  3  | <p align='left'> 0 |\n",
    "| <p align='left'> 1. Vizualizing the Data                | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. Formatting Input Data               | <p align='left'>  3  | <p align='left'> 1 |\n",
    "| <p align='left'> 3. Building a DNN model                | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 4. Understanding our model             | <p align='left'>  5  | <p align='left'> 2 |\n",
    "| <p align='left'> 5. Training our model                  | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 6. Understanding the training history  | <p align='left'>  3  | <p align='left'> 1 |\n",
    "| <p align='left'> 7. Making a Prediction                 | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 8. Testing the model                   | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 9. Making Predictions with new data    | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> **Bonus 1:** Analyzing Weights         | <p align='left'>  5  | <p align='left'> 1 |\n",
    "| <p align='left'> **Bonus 2:** Loading a model and optimizing it further | <p align='left'>  7  | <p align='left'> 1 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's keep our keras backend tensorflow quiet\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# if you need to force tensorflow to use your CPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Importing tensorflow so we can access anything later\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# See how many GPU's are available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# keras imports for importing our dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "# keras imports for building our neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "# keras import for manipulating some of our data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# keras tools for loading an image from disk\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# keras tools for loading an ANN model from disk\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec01e96",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 0: Understanding Fitting\n",
    "\n",
    "### Problems when fitting (an interlude)\n",
    "\n",
    "When fitting to a dataset we trust that the algorithm that we're using will find the correct minima and that the fit hasn't become pronned to some problem such as vanishing gradients.\n",
    "\n",
    "Here we're going to demonstrate a quick example of problems that you can hit with a fit failing to converge.\n",
    "\n",
    "There are many tools/checks in place in most modern libraries to help fix common problems, but we will explicitly use the `SLSQP` method here which allows us to look at narrow ranges of a full function.\n",
    "\n",
    "`result = minimize(objective, pt, method='SLSQP', jac=derivative, bounds=[(...),])`\n",
    "\n",
    "The objective function of this fit has been provided it is the equation of the form `y=x^4-5x^2+0.1x` .\n",
    "\n",
    "This function has 3 points where the gradient is zero and has 2 minima, a global minima and a local minima.\n",
    "\n",
    "## 0.1 Provide the derivative function for this 1D method (this is the Jacobian)\n",
    "\n",
    "This is just a case of returning the correct value from the derivative method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(x):\n",
    "  return x[0]**4 - 5*x[0]**2 + 0.1*x[0]\n",
    "\n",
    "# Part 1 complete the derivative function\n",
    "def derivative(x):\n",
    "  return 4*x[0]**3 - 10*x[0] + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e42b37",
   "metadata": {},
   "source": [
    "## 0.2 Run the minimize method over given ranges and identify the 3 'stable' points of the function\n",
    "\n",
    "Run the minimize method over these 3 ranges:\n",
    "\n",
    "| <p align='left'> Ranges |\n",
    "| :--- |\n",
    "| ` (0.1, 5.0)` |\n",
    "| ` (0.0, 5.0)`  |\n",
    "| `(-5.0, 5.0)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf94f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = (5.,)\n",
    "ranges = [(0.1, 5.0),  (0.0, 5.0),  (-5.0, 5.0)]\n",
    "METHOD = \"SLSQP\"\n",
    "\n",
    "# Part 2 perform 3 minimizations\n",
    "result_1 = minimize( objective, pt, method=METHOD, jac=derivative, bounds=[ranges[0]] )\n",
    "result_2 = minimize( objective, pt, method=METHOD, jac=derivative, bounds=[ranges[1]] )\n",
    "result_3 = minimize( objective, pt, method=METHOD, jac=derivative, bounds=[ranges[2]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd34b65",
   "metadata": {},
   "source": [
    "## 0.3 Identify the true minima of the function\n",
    "\n",
    "##### Hint:\n",
    "    \n",
    "Running `scipy.minimize` returns an object containing the fit result and a bunch of extra output.\n",
    "The value of `x` from a 'minimize' after it has finished running can be accessed via `result['x']` and the function value can be accessed via `result['fun']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum 1: {}'.format(result_1['x']))\n",
    "print('Minimum 2: {}'.format(result_2['x']))\n",
    "print('Minimum 3: {}'.format(result_3['x']))\n",
    "\n",
    "print('Result 1: {}'.format(result_1['fun']))\n",
    "print('Result 2: {}'.format(result_2['fun']))\n",
    "print('Result 3: {}'.format(result_3['fun']))\n",
    "\n",
    "# Part 3 identify the true minima\n",
    "results = {\n",
    "    \"Result 1\":result_1['fun'],\n",
    "    \"Result 2\":result_2['fun'],\n",
    "    \"Result 3\":result_3['fun'],\n",
    "}\n",
    "\n",
    "print(f'Best Result is: {min(results, key=results.get)}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e051f",
   "metadata": {},
   "source": [
    "# Load & Visualize the dataset\n",
    "\n",
    "#### Loading the data\n",
    "\n",
    "The data we're using comes from the well use mnist dataset which has been around since the 80s and is something people new to AI/ML often try their hand at building models for.\n",
    "\n",
    "###### About the data\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data\n",
    "\n",
    "This dataset consists of many pixelated digitized images of hand-written numbers.\n",
    "These images contain data in a single channel i.e. black and white images. White images on a Black background.\n",
    "\n",
    "\n",
    "We've loaded the mnist class and we just need to call `load_data` to get the dataset in order for us to use this data.\n",
    "\n",
    "This method returns 2 tuples. The training data and labels as well as the test data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12460d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Training and Test data\n",
    "(X_train, Y_train), (X_test, Y_test) =  mnist.load_data()\n",
    "\n",
    "print(f\"Images are a {type(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0718970",
   "metadata": {},
   "source": [
    "# Visualize the dataset\n",
    "\n",
    "It's important to understand the data that you're trying to train on. In most cases preparing the data in some way that it's suitable for training is the most time consuming part of building and training a model to be useful.\n",
    "\n",
    "# Problem 1: Understanding the input data\n",
    "\n",
    "Now that we've loaded the data we need to make sure that it's in a format that we can use.\n",
    "\n",
    "## 1.1 First, What is the shape of an individual image?\n",
    "Firstly, what is the shape of the each input image?, and what range of values do the individual pixels within the image take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to normalise Images \n",
    "\"\"\"\n",
    "def normalise_dataset(dataset, mean, std, max_value=255.0):\n",
    "    dataset = ( (dataset/max_value) - mean )/std\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_number, hight, width = X_train.shape\n",
    "pixel_value_min = X_train.min()\n",
    "pixel_value_max = X_train.max()\n",
    "lable_value_max = Y_train.max()\n",
    "lable_value_min = Y_train.min()\n",
    "\n",
    "print(f\"The shape of the MNIST dataset is {train_sample_number}x{hight}x{width}.\")\n",
    "print(f\"Minimum pixel value is {pixel_value_min} and maximum is {pixel_value_max}.\")\n",
    "print(f\"Minimum lable value is {lable_value_min} and maximum is {lable_value_max}.\")\n",
    "print(f\"These is the typical pixel range for the PNG format.\")\n",
    "print(f\"Tensor dtype: {X_train.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b222e4",
   "metadata": {},
   "source": [
    "## 1.2 Plotting some of the input data\n",
    "\n",
    "Secondly, we should attempt to plot some of the input data to our fit to understand what the data looks like.\n",
    "It is possible to use the pyplot method to plot the images which we have just loaded into tensors.\n",
    "\n",
    "##### Hint:\n",
    "The options of `cmap='gray', interpolation='none'` with `pyplot.imshow` show the images as we would expect to see them.\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "\n",
    "#### Plot the Data\n",
    "\n",
    "###### Plot the first 2 values from the training portion of the dataset using the values to the first 2 training labels as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "axes = (ax1, ax2)\n",
    "for idx in range(len(axes)):\n",
    "    axes[idx].imshow(X_train[idx], cmap=\"gray\", interpolation=\"none\")\n",
    "    axes[idx].set_title(f\"MNIST lable: {Y_train[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb09b4",
   "metadata": {},
   "source": [
    "# Normalizing and Formatting the dataset\n",
    "\n",
    "Before we begin fitting the dataset it is useful to make sure that the data is in a format that we can use for fitting.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 2: Formatting Input Data\n",
    "\n",
    "\n",
    "### 2.1 Normalize and reformat our Input Data.\n",
    "\n",
    "For our model we're going to use a flat array of 768 pixel values as our input and we want to make sure that our data has been correctly normalized so that the values are all in the unitary range [0,1].\n",
    "\n",
    "The data should be converted to the `'float32'` using the `astype` method and then normalized to have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into a float from an integer array\n",
    "X_train = tf.cast(X_train, tf.float32)\n",
    "X_test = tf.cast(X_test, tf.float32)\n",
    "Y_train = tf.cast(Y_train, tf.float32)\n",
    "Y_test = tf.cast(Y_test, tf.float32)\n",
    "\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train = normalise_dataset(X_train, mean=0.5, std=0.5)\n",
    "X_test = normalise_dataset(X_test, mean=0.5, std=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dbcae",
   "metadata": {},
   "source": [
    "## 2.2 Resize the input data\n",
    "The input data should be re-formatted such that we have 1 flat array of inputs for each image.\n",
    "\n",
    "i.e. we have 60,000 images for training, we want each of these images to contain data in a flat array which means we are going from a 3D tensor to a 2D tensor. e.g. `(60000, 10, 10) ==> (60000, 100)` \n",
    "\n",
    "##### Hint: Tensor.Reshape can be used to simplify the re-shaping/re-sizing of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## building the input 1D vector from the 28x28 pixels\n",
    "\n",
    "X_train = tf.reshape(X_train, shape=[X_train.shape[0], -1])\n",
    "X_test = tf.reshape(X_test, shape=[X_test.shape[0], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69060a15",
   "metadata": {},
   "source": [
    "## 2.3 Change our input labels for fitting\n",
    "\n",
    "Now we need to make sure that the dataset labels are in a format which we can use alongside the training and testing dataset.\n",
    "\n",
    "Typically DNN expect category labels to be in a one-hot encoding. This means that labels are vectors with a single entry of 1 and all other entries being 0. \n",
    "\n",
    "##### Hint: This can be achieved using the `to_categorical` utility function in Keras.\n",
    "\n",
    "Using this change the Y_test and Y_train labels to be cateory labels.\n",
    "\n",
    "If you want to be explicit we know we're working with `n_classes = 10` due to looking at the numerical characters `0...9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print value of label prior to one hot encoding\n",
    "print(Y_train[0])\n",
    "print(Y_test[0])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes=10, dtype=\"float32\")\n",
    "Y_test = to_categorical(Y_test, num_classes=10, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print lable values after one hot encoding\n",
    "print(Y_train[0])\n",
    "print(Y_test[0])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28bb7a",
   "metadata": {},
   "source": [
    "# Problem 3: Building the DNN\n",
    "\n",
    "\n",
    "In Tensorflow a 'standard' DNN model is defined using the Sequential model.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "\n",
    "Then additional layers are added to the model one at a time until the model is complete using `model.add`.\n",
    "\n",
    "\n",
    "## Build a DNN with 2 \"hidden layers\"\n",
    "\n",
    "You should build a DNN such that it has 2 hidden and 1 output layer. When building a model we have neurons and activation functions. These are 'layers' in TensorFlow but we want 3 layers with neurons and activators such as: \n",
    "\n",
    "| <p align='left'> Layer |\n",
    "| :--- |\n",
    "| Dense |\n",
    "| Activation |\n",
    "| Dense |\n",
    "| Activation |\n",
    "| Dense |\n",
    "| Activation |\n",
    "\n",
    "A flat layer of neurons is defined using the `Dense` layer in Tensorflow.\n",
    "For our example we want to keep our Activators and our layers of neurons separate.\n",
    "    \n",
    "We want to use a random flat distribution of weights when initializing the parameters in this fit,\n",
    "    \n",
    "### !!Use the parameters `use_bias=True, kernel_initializer='RandomUniform'` when constructing a layer of neurons!!\n",
    "\n",
    "###### NB: By default each Dense layer also applies a bias to each node to improve the fit performance.\n",
    "    \n",
    "For our hidden nodes we want to use the `'GELU'` Activator for this example and `'softmax'` to build our a numerical classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba268e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyper Parameters (ish)\n",
    "\"\"\"\n",
    "n_categories = 10\n",
    "n_hidden_nodes_1 = 256\n",
    "n_hidden_nodes_2 = 512\n",
    "hidden_activation_type = 'gelu'\n",
    "output_activation_type = \"softmax\"\n",
    "weight_start_values = 'RandomUniform'\n",
    "\n",
    "\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(\n",
    "     units = n_hidden_nodes_1,\n",
    "     use_bias = True,\n",
    "     kernel_initializer = weight_start_values   \n",
    "))\n",
    "model.add(Activation(hidden_activation_type))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(\n",
    "     units = n_hidden_nodes_1,\n",
    "     use_bias = True,\n",
    "     kernel_initializer = weight_start_values   \n",
    "))\n",
    "model.add(Activation(hidden_activation_type))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(\n",
    "     units = n_categories,\n",
    "     use_bias = True,\n",
    "     kernel_initializer = weight_start_values   \n",
    "))\n",
    "model.add(Activation(output_activation_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e825c92",
   "metadata": {},
   "source": [
    "## Compiling our model\n",
    "\n",
    "At this point we have a description of our model. Now we want to compile it.\n",
    "\n",
    "Strictly speaking some of the steps here are optional, but we want to be explicit with how we're building our model to be sure that we've understood everything that is going on.\n",
    "\n",
    "We will be using `categorical_crossentropy` as discussed in our lecture as our loss model.\n",
    "\n",
    "We will also be using the `Stochastic Gradient Descent` or `SGD` optimizer for our training. This is different to the default.\n",
    "\n",
    "`metrics=['accuracy']` is also passed to our model to ensure that it collects training data that we're interested in later on.\n",
    "\n",
    "\n",
    "## Now compile and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3deac8-536b-4262-93a6-0479dc1bb53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='SGD')\n",
    "# Now we want to 'build' the model, so tell it about our input data format\n",
    "model.build(input_shape=(1,784))\n",
    "\n",
    "# Print a helpful summary of our model\n",
    "model.summary()\n",
    "\n",
    "# Now we're going to take a copy of the weights in our first hidden layer of the fit\n",
    "weights_before_training = model.layers[0].get_weights()[0].copy()\n",
    "weights_before_training.resize(200704)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3ef3d",
   "metadata": {},
   "source": [
    "# Problem 4: Understanding our model\n",
    "\n",
    "## Q: 4.1\n",
    "\n",
    "##### Using an Input Shape of `784` and a hidden layer of `256` nodes you should see `200960` free parameters in the first layer of your model.\n",
    "\n",
    "##### Explain where this number of free parameters comes from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee2c65",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The number of free parameters per layer is given by ther number of weights in a layer + the number of biases in a layer. \n",
    "\n",
    "For a linear feed-forward network, the number of weights in a layer in given by the product of input nodes and output nodes and the number of biases is given by the number of nodes in the output, hence the total number of free parameters is given by $N_{param} = (N_{in}*N_{out}) + N_{out}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad93994",
   "metadata": {},
   "source": [
    "## Q: 4.2\n",
    "\n",
    "##### Using `256` nodes per hidden layer there will also be `65792` weights between the 2 hidden layers in the model.\n",
    "\n",
    "##### Explain where these weights come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd4150",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "See answer from question 4.1. Same answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47c4f2",
   "metadata": {},
   "source": [
    "## Q: 4.3\n",
    "\n",
    "##### Explain why a `'softmax'` activation function is used at the end of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735055f0",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Our outputs must be the degree of belief of the model that an image to be a specific lable, hence we wish our outputs  to be probabilities bounded to the range $\\in [0, 1]$. Hence as softmax activation function is also limited to this range, we use this for our output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3c156",
   "metadata": {},
   "source": [
    "## Q: 4.4\n",
    "\n",
    "##### How many weights would we expect in the whole model when using 128 neurons per hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486e255",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Using equation from above: $(128*128) + 128 = 16512$ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d623e92",
   "metadata": {},
   "source": [
    "## Q: 4.5\n",
    "\n",
    "##### Why are activation functions needed in a DNN which is trained on data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846da7a7",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Dense layers are simply linear layers with linear functions being applied at each node. In order for the outputs of our layers to be non-linear, we require non-linear activtion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d1791",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 5: Train the model\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Now that we have a model we want to fit we now want to train it on our training dataset making sure to use our validation data to check on our training.\n",
    "\n",
    "The result of this training will be stored in our history and will allow us to go back and asses how well our training worked.\n",
    "\n",
    "\n",
    "## 5.1 Perform the training\n",
    "Complete and run the command below to train the model on data using a `batch_size` of 600 for 100 `epochs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c7124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# training the model and saving metrics in history\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=2,\n",
    "          batch_size=600,\n",
    "          epochs=100,\n",
    "          callbacks=[tensorboard_callback]\n",
    "        )\n",
    "\n",
    "## Now take a copy of the weights in the first hidden layer _after_ training on data\n",
    "weights_after_training = model.layers[0].get_weights()[0].copy()\n",
    "weights_after_training.resize(200704)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49b609",
   "metadata": {},
   "source": [
    "## Q: 5.2\n",
    "\n",
    "##### Why are there 100 steps per epoc in this training? (0.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849846dd",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Because since we have decided to use batch training with a batch size of 600, we end up with 60000/600 = 100 mini batches. We iterate over each mini-batch during every epoch in the training phase, hence there will be 100 iterations of training per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab5c0a",
   "metadata": {},
   "source": [
    "\n",
    "# Problem6: Understanding the training history\n",
    "\n",
    "### Cross-Checking our Model fit\n",
    "\n",
    "The returned history from our fit contains both the accuracy and the loss function over the epocs of training.\n",
    "\n",
    "This information can be accessed via: `history.history[]`\n",
    "\n",
    "We now want to make 2 plots:\n",
    "\n",
    "\n",
    "## 6.1 Plotting Evolution of model Accuracy\n",
    "\n",
    "This plot will show how the accuracy of the model has increased vs epocs of training.\n",
    "\n",
    "This should show for a good model that accuracy increases with the number of epocs.\n",
    "\n",
    "###### Hint:\n",
    "`history.history[accuracy]` vs `history.history[val_accuracy]` give the model accuracy during the training epocs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_training = history.history['accuracy']\n",
    "accuracy_test = history.history['val_accuracy']\n",
    "\n",
    "print(f\"The accuracy of the model using the training dataset is {accuracy_training[-1]}\")\n",
    "print(f\"The accuracy of the model using the test dataset is {accuracy_test[-1]}\")\n",
    "\n",
    "plt.plot(accuracy_test, label=\"test\", c=\"r\")\n",
    "plt.plot(accuracy_training, label=\"train\", c=\"b\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy of feed-forward linear network\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3e7ac",
   "metadata": {},
   "source": [
    "## 6.2 Plotting Evolution of model losses\n",
    "\n",
    "As above, but now we want to see how the loss function evolves using the `loss` vs `val_loss` parameters vs epoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a49dc-73b9-4c5e-9a81-4e0e0dd73975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "print(f\"The accuracy of the model using the training dataset is {training_loss[-1]}\")\n",
    "print(f\"The accuracy of the model using the test dataset is {test_loss[-1]}\")\n",
    "\n",
    "plt.plot(test_loss, label=\"test\", c=\"r\")\n",
    "plt.plot(training_loss, label=\"train\", c=\"b\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss function\")\n",
    "plt.title(\"Loss of feed-forward linear network\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf627c",
   "metadata": {},
   "source": [
    "## Q 6.3: Is this model over-trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76966f52",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The difference in the accuracy of the model on the training dataset (which it has knowledge of) and the test dataset (which it does not) is in the order of $O(10^{-3})$, hence its performance is almost identical on both datasets.\n",
    "\n",
    "This is indication that the model is sufficiently generalised and therefore is not over trained. \n",
    "\n",
    "The same conclusion is supported when looking at the loss of the model at the end of training, with no significant difference in loss between both datasets.\n",
    "\n",
    "Additionally, while the slope of the of the loss curve is small, it has not plataued for a significantly, indicating that perhapse we may still be able to improve the preformance of the model slightly with some additional epochs of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894d0cc",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 7: Making predictions\n",
    "\n",
    "### Testing how well our model classifies different numbers\n",
    "\n",
    "In order to test how well our model actually classifies different numbers we need to be able to make predictions using it.\n",
    "\n",
    "Our model is designed to output the probability of each label being true (according to the model).\n",
    "\n",
    "With that in mind we want to get the best value from a single image.\n",
    "\n",
    "\n",
    "Our model has been defined to take an input of shape `(1, 784)`. This allows us to pass many images at a time to the model to either train or to have predictions made on.\n",
    "\n",
    "\n",
    "## 7.1 Predicting with our model\n",
    "\n",
    "### Want to make a prediction using the 10th element of the testing dataset\n",
    "\n",
    "Lets first make a single prediction using our dataset and model.\n",
    "\n",
    "To do this we need to:\n",
    "\n",
    "1. Take the 10th element from our test dataset\n",
    "2. Resize this to match the expected input dimensions from our model\n",
    "3. Make a prediction using this input using `model.predict`\n",
    "4. Make a bar chart showing the predictions from the model.\n",
    "\n",
    "###### Hint: `plt.bar` is useful for plotting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca3e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_prediction_input = X_test[9]\n",
    "# Expand 0th dimention to get shape (1, 784)\n",
    "single_prediction_input = tf.expand_dims(single_prediction_input, axis=0)\n",
    "prediction = model.predict(single_prediction_input)\n",
    "\n",
    "labels = np.arange(0, 10)\n",
    "plt.bar(labels, prediction[0])\n",
    "plt.title(\"Neural network prediction on input\")\n",
    "plt.xticks(labels)\n",
    "plt.xlabel(\"labels\")\n",
    "plt.ylabel(\"Probability of prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c132c2",
   "metadata": {},
   "source": [
    "## 7.2 Making a Numerical Prediction\n",
    "\n",
    "The method `argmax` from numpy can help us change this prediction distribution from a vector to a single value.\n",
    "\n",
    "### Predict the number in this image ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858db180",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The ground truth label is {np.argmax(Y_test[9])}\")\n",
    "print(f\"The model predicts the input to be {np.argmax(prediction[0])} with a probability of {np.max(prediction[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265939e0",
   "metadata": {},
   "source": [
    "# Problem 8 Testing the model\n",
    "\n",
    "One of the ways we can evaluate how well our model is spearating our input data we can build a confusion matrix.\n",
    "\n",
    "This allows us to see how many numbers are incorrectly identified using a labelled dataset.\n",
    "\n",
    "## Constructing a Confusion Matrix\n",
    "\n",
    "In order to construct a confusion matrix we need to make predictions over a large number of inputs and be able to compare them to labels which are known to be correct.\n",
    "\n",
    "Taking the 'training' dataset can collect numerical predictions using `argmax` for each of the 60,000 entries in the dataset.\n",
    "\n",
    "## 8.1 Make many predictions\n",
    "\n",
    "To collect enough data to produce our confusion matrix we can make predictions over the whole training (or test) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using test dataset to not bias the results\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Find labels for each datapoint\n",
    "test_truth_labels = Y_test.argmax(axis=1)\n",
    "test_truth_predictions = test_predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cff0da",
   "metadata": {},
   "source": [
    "## 8.2 Plot Confusion Matrix\n",
    "\n",
    "We can now construct a confusion matrix using `tf.math.confusion_matrix` and plot it with `sns.heatmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c9f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the confusion matrix\n",
    "confusion_matrix = tf.math.confusion_matrix(\n",
    "    labels=test_truth_labels,\n",
    "    predictions=test_truth_predictions,\n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "# Import the useful tools for plotting a confusion matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Plot the confusion matrix using a heatmap\n",
    "sns.heatmap(confusion_matrix, annot=False, cmap='Blues', norm=LogNorm())\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfdbd9",
   "metadata": {},
   "source": [
    "# 9: Making Predictions with new data\n",
    "\n",
    "Make aure that you download the sample image \n",
    "\n",
    "## 9.1 Load data from `sample_image.png`\n",
    "\n",
    "Complete the load_image method below to load a new numerical image file from disk, resize and normalize the data into a format which we can use with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "    # load the image\n",
    "    #img = load_img(filename, target_size=(28, 28), color_mode='grayscale')\n",
    "    img = load_img(filename, target_size=(28, 28), color_mode='grayscale')\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    " \n",
    "    # Cast to tensor \n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # Normalise\n",
    "    img = normalise_dataset(img, mean=0.5, std=0.5)\n",
    "    # Resize\n",
    "    img = tf.reshape(img, shape=[1, -1]) \n",
    "    return img\n",
    "\n",
    "# load the image\n",
    "img = load_image('./sample_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bf6ba",
   "metadata": {},
   "source": [
    "## 9.2 Make a prediction on the new data\n",
    "\n",
    "Now that we've loaded the sample file, use our model to make a prediction on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class\n",
    "image_prediction = model.predict(img)\n",
    "print(image_prediction)\n",
    "digit = argmax(image_prediction)\n",
    "print('Prediction: {}'.format(digit))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(mpimg.imread('./sample_image.png'), cmap='gray', interpolation='none')\n",
    "plt.title(\"Sample File\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "\n",
    "plt.bar(labels, image_prediction[0])\n",
    "plt.yscale('log')\n",
    "plt.xticks(np.arange(0,10))\n",
    "\n",
    "plt.title(\"Predicted Value Distribution\")\n",
    "plt.xlabel(\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619684a",
   "metadata": {},
   "source": [
    "# Bonus 1: Analyzing Weights\n",
    "\n",
    "Before and after fitting our model we took copies of the weights used in training our dataset.\n",
    "\n",
    "If you construct a histogram of the weights from before fitting what distribution does it follow?\n",
    "\n",
    "## B1.1 Plot weights from before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319e643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "_, _, _ = plt.hist(weights_before_training, bins=500)\n",
    "print(f\"Uniform distribution. Randomly initialised weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e19657",
   "metadata": {},
   "source": [
    "## B1.2 Plot the weights After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccd256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "_, _, _ = plt.hist(weights_after_training, bins=500)\n",
    "print(f\"Uniform distribution convoluted with a flat/rounded gaussian. GELU as activation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5ec28",
   "metadata": {},
   "source": [
    "## B1.3 Compare the weights\n",
    "\n",
    "Construct a plot comparing the weights before fitting to the values after fitting.\n",
    "\n",
    "##### Hint: You may need to use a log plot in y to see the full distribution(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba82be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(weights_after_training, bins=100)\n",
    "plt.hist(weights_before_training, bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02b2d4",
   "metadata": {},
   "source": [
    "## B1.4 Q: How might the weights vary if you train the model for an additional 80 steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b75f3b",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "We would expect the the weights to be changes even more, the distribution of the weights will start to resemble more the underlying activation function used,hence for GELU, we shoud expect to see a higher proportion of the weights to fall within $1\\sigma$ of the distribution and fewer weights at the tail of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cab702",
   "metadata": {},
   "source": [
    "## B1.5 Q: How would including a regularizer impact the distribution of weights after training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d45f7",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "### FINISH ME ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59248c16",
   "metadata": {},
   "source": [
    "# Bonus 2: Loading a model and optimizing it further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8c458",
   "metadata": {},
   "source": [
    "## B2.1 Saving our model\n",
    "\n",
    "Lets save our model to disk so we can take it home and use it to identify numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_path = './keras_mnist.keras'\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424cc24",
   "metadata": {},
   "source": [
    "## B2.2 Lets load our model and do some further training\n",
    "\n",
    "We can load a keras model from disk using the `'load_model'` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1c9f7-6ad6-44e8-9ce2-593995954bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "nu_model = load_model('./keras_mnist.keras')\n",
    "nu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e1c59",
   "metadata": {},
   "source": [
    "## B2.3 Make some small changes this time\n",
    "\n",
    "Now we want to try training using the `'Adam'` optimizer so re-compile the model.\n",
    "\n",
    "(If you re-run model.summary you will find it doesn't need to be re-built)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "nu_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adam')\n",
    "\n",
    "# Print a helpful summary of our model\n",
    "nu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88cd17",
   "metadata": {},
   "source": [
    "## B2.4 Re-Train\n",
    "\n",
    "We don't have new data to train our model on, but what happens if we continue training with our existing dataset?\n",
    "\n",
    "Re-train as before with the same data and batch/epoch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model and saving metrics in history\n",
    "nu_history = nu_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=2,\n",
    "          batch_size=600,\n",
    "          epochs=30\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1290e",
   "metadata": {},
   "source": [
    "## B2.5 Evaluating Extended Model Accuracy\n",
    "\n",
    "Lets make another set of plots to compare how our model accuracy improved with additional training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f3c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_training = nu_history.history['accuracy']\n",
    "accuracy_test = nu_history.history['val_accuracy']\n",
    "\n",
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(accuracy_training)\n",
    "plt.plot(accuracy_test)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de963f38",
   "metadata": {},
   "source": [
    "## B2.6 Compare Extended loss functions\n",
    "\n",
    "As above lets generate some new plots for our loss function evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac801c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = nu_history.history['loss']\n",
    "validation_loss = nu_history.history['val_loss']\n",
    "\n",
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(training_loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acf560",
   "metadata": {},
   "source": [
    "## B2.7 Have we over-trained?\n",
    "\n",
    "Using the above plots do you think we've now over-trained our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6f3cd",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "We see both in the loss plot and accuracy plot a large difference between the preformance of the network on the training dataset compared to the validation dataset (good preformance in train, worse preformance in val).\n",
    "\n",
    "This is a likely indication that the model has learned to fit the statistical noise present in the distribution of the training dataset, reducing the generalisability of the model with data outside of the training dataset. This is a clear sign of over-training/over-fitting.\n",
    "\n",
    "Conclusion we can draw from this: When it comes to classifying MNIST with a linear feed-forward network, SGD optimiser preformes better than Adam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
