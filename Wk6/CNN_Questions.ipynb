{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJMA88F7g0DB"
   },
   "source": [
    "# Convolution Deep Neural Networks\n",
    "---\n",
    "*Responsible:* Robert Currie (<rob.currie@ed.ac.uk>)\n",
    "\n",
    "### Description:\n",
    "This is a longer notebook than we have had previously. It is intended as a hands-on example to using CNN/CDN models in different situations.\n",
    "\n",
    "There are 4 sections:\n",
    "\n",
    "**Image Filtering**- This section is intended to give you an idea how the Sobel Operator works.\n",
    "\n",
    "**CNN training with cifar10** - This section is intended to give an example of training a CNN model to a dataset. This also shows you how to avoid problems due to a complex model and a limited dataset size.\n",
    "\n",
    "**VAE examples with mnist** - This section is intended to give you an example of using a CNN VAE model to perform an un-supervised training over a dataset.\n",
    "\n",
    "**Anomaly Detection** - This section shows how you can make use of the latent-space of these models to perform anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking\n",
    "\n",
    "As with last weeks notebook the sections marked **##FINISH ME##** need to be completed for the notebook to work.\n",
    "\n",
    "Marks for the different parts are shown below.\n",
    "\n",
    "* Sections are intended to be tackled in order, i.e. 1->9\n",
    "* In this notebook different sections can be tackled independently\n",
    "* There are bonus problems at the end to tackle but the maximum mark is 10/10\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Image Filtering (question)          | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. CNN training with cifar10 (code)     | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. CNN training with cifar10 (questions)| <p align='left'>  4  | <p align='left'> 4 |\n",
    "| <p align='left'> 3. VAE examples with mnist (sections marked Q) | <p align='left'>  4  | <p align='left'> 4 |\n",
    "| <p align='left'> 4. **Bonus:** Anomaly Detection            | <p align='left'>  3  | <p align='left'> 3 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup our notebook env\n",
    "---\n",
    "Import all of the dependencies for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras import models, layers, datasets, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, InputLayer, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Filtering\n",
    "---\n",
    "We're now going to go through an example of applying an image filter operator on an input image.\n",
    "\n",
    "In reality we normally tend to use libraries which accelerate this rather than doing these calculations in a higher level language such as Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = 'image.png'\n",
    "input_image = imread(image_file)  # this is the array representation of the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot our Input\n",
    "---\n",
    "Lets have a look at our input image from wikipedia: https://en.wikipedia.org/wiki/File:Eilean_Donan_Castle,_Scotland_-_Jan_2011.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(input_image, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to grayscale\n",
    "---\n",
    "This short section of code downsamples our input image to greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting each one of the RGB components\n",
    "r_img, g_img, b_img = input_image[:, :, 0], input_image[:, :, 1], input_image[:, :, 2]\n",
    "# The following operation will take weights and parameters to convert the color image to grayscale\n",
    "gamma = 1.400  # a parameter\n",
    "r_const, g_const, b_const = 0.2126, 0.7152, 0.0722  # weights for the RGB components respectively\n",
    "grayscale_image = r_const * r_img ** gamma + g_const * g_img ** gamma + b_const * b_img ** gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our sobel operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our Sobel operators\n",
    "\n",
    "```\n",
    "The kernels Gx and Gy as covered in the last lecture:\n",
    "      _               _                   _                _\n",
    "     |                 |                 |                  |\n",
    "     | 1.0   0.0  -1.0 |                 |  1.0   2.0   1.0 |\n",
    "Gx = | 2.0   0.0  -2.0 |    and     Gy = |  0.0   0.0   0.0 |\n",
    "     | 1.0   0.0  -1.0 |                 | -1.0  -2.0  -1.0 |\n",
    "     |_               _|                 |_                _|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the matrices associated with the Sobel filter\n",
    "Gx = np.array([\n",
    "    1.0, 0.0, -1.0,\n",
    "    2.0, 0.0, -2.0,\n",
    "    1.0, 0.0, -1.0,\n",
    "    ])\n",
    "Gy = np.array([\n",
    "    1.0, 2.0, 1.0,\n",
    "    0.0, 0.0, 0.0,\n",
    "    -1.0, -2.0, -1.0,\n",
    "    ])\n",
    "\n",
    "[rows, columns] = np.shape(grayscale_image)  # we need to know the shape of the input grayscale image\n",
    "\n",
    "sobel_filtered_image_x = np.zeros(shape=(rows, columns))  # initialization of the output image array (all elements are 0)\n",
    "sobel_filtered_image_y = np.zeros(shape=(rows, columns))  # initialization of the output image array (all elements are 0)\n",
    "sobel_filtered_image = np.zeros(shape=(rows, columns))  # initialization of the output image array (all elements are 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the Operator to our input\n",
    "\n",
    "A suitable example method for applying the full sobel operator to our input data is:\n",
    "```\n",
    "# Now we \"sweep\" the image in both x and y directions and compute the output\n",
    "for i in range(rows - 2):\n",
    "    for j in range(columns - 2):\n",
    "        gx = np.sum(np.multiply(Gx, grayscale_image[i:i + 3, j:j + 3]))  # x direction\n",
    "        gy = np.sum(np.multiply(Gy, grayscale_image[i:i + 3, j:j + 3]))  # y direction\n",
    "\n",
    "        sobel_filtered_image[i + 1, j + 1] = np.sqrt(gx ** 2 + gy ** 2)  # calculate the \"hypotenuse\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we \"sweep\" the image in both x and y directions and compute the output\n",
    "for i in range(rows - 2):\n",
    "    for j in range(columns - 2):\n",
    "        gx = np.sum(np.multiply(Gx.reshape(3,3), grayscale_image[i:i + 3, j:j + 3]))  # x direction\n",
    "        gy = np.sum(np.multiply(Gy.reshape(3,3), grayscale_image[i:i + 3, j:j + 3]))  # y direction\n",
    "\n",
    "        sobel_filtered_image_x[i+1, j+1] = gx\n",
    "        sobel_filtered_image_y[i+1, j+1] = gy\n",
    "        sobel_filtered_image[i + 1, j + 1] = np.sqrt(gx ** 2 + gy ** 2)  # calculate the \"hypotenuse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Plot our output\n",
    "\n",
    "Plot the output of applying G_x, G_y and G operators each on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(sobel_filtered_image_x, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(sobel_filtered_image_y, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(sobel_filtered_image, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the difference between the G_x and G operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Sobel operators are used to find edges in images by looking at their second derivatives in a certain direction. In which, the two Sobel operators, one of them being $G_x$, is capable of looking for and separating out edges in the horizontal direction, and effectively ignoring any large differences in pixels in the vertical direction.\n",
    " \n",
    "Where as the $G$ operator is effectively a combination of the horizontal and vertical Sobel operator $G_x$ and $G_y$, respectively, which the operator $G$ effectively finds the edges of the image, combining both the horizontal and the vertical, or \"sweeping\" the image for edges. And it is calculated as a \"hypotenuse\", using Pythagoras, or: \n",
    " $$\n",
    " G = \\sqrt{G_x^2 + G_y^2}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training with cifar10\n",
    "---\n",
    "\n",
    "This section gives an example of using a CNN to train on the cifar10 dataset.\n",
    "\n",
    "The cifar10 dataset is a dataset of labelled images which is composed of the categories `['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']`.\n",
    "\n",
    "As the dataset is composed of colour images, the data is composed of pixels in 3 dimensions, (r,g,b) which are used to construct a full colour image.\n",
    "\n",
    "This dataset contains a lot of low-resolution images that have been hand labelled, however, this is a finite dataset with quite a few categories which means training to the dataset can be tricky. Typically a lot of simple CNN models tend to reach a non-overfitted accuracy of 70-80%.\n",
    "\n",
    "\n",
    "Lets start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#hyper-parameters\n",
    "BATCH_SIZE = 600\n",
    "nb_epochs = 15\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_classes = 10\n",
    "\n",
    "# dataset \n",
    "num_train, img_channels, img_rows, img_cols = X_train.shape\n",
    "num_test, _, _, _= X_test.shape\n",
    "class_names =['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some examples from our input\n",
    "---\n",
    "Plot a single example from each of the classes in the dataset and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax0, ax1, ax2, ax3, ax4,),(ax5, ax6, ax7, ax8, ax9)) = plt.subplots(2, 5)\n",
    "axes = (ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    image = X_test[np.argwhere(y_test==i)[0,0]]\n",
    "    axes[i].imshow(image,)\n",
    "    axes[i].set_title(class_names[i])\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're going to Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to categorical\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "max_pixel_value = X_train.max()\n",
    "\n",
    "#float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= max_pixel_value.astype('float32') #Normalize\n",
    "X_test /= max_pixel_value.astype('float32')  #Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a CNN model using the Sequential API which has:\n",
    "```\n",
    "1. InputLayer\n",
    "2. Conv2D x 32 with kernel 3x3 and strides=2\n",
    "3. ReLU\n",
    "4. Conv2D x 128 with kernel 3x3 and strides=1\n",
    "5. ReLU\n",
    "6. Flatten\n",
    "7. Dense x 1024\n",
    "8. ReLU\n",
    "9. Dense x 1024\n",
    "10. ReLU\n",
    "11. Dense x classes\n",
    "12. softmax\n",
    "```\n",
    "\n",
    "This model will be used multiple times so we want to wrap it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add( InputLayer(input_shape=X_train.shape[1:]) )\n",
    "    model.add( Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same') )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\") )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Flatten() )\n",
    "    model.add( Dense(1024,) )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Dense(1024,) )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Dense(num_classes) )\n",
    "    model.add( Activation('softmax') )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_model()\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "---\n",
    "\n",
    "As per training a DNN to the mnist dataset we want to train our CDN to our cifar10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = nb_epochs, validation_data = (X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now examine the loss functions from this fit\n",
    "---\n",
    "\n",
    "Plot the model losses for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Has the model converged to a good description of the dataset? If not why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. As observing from the loss distribution of the model across the epochs, it seems, alone from the training dataset, the model is converging towards a good description of the dataset. But, if one take into account of the validation loss, one would observe that it is apparent that the validation loss is diverging from the training loss.\n",
    "\n",
    "This can be the result of overtraining of the model, which, at the point the model can no longer be generalized to dataset outside of the training dataset. The reason for the overtraining can potentially be the sample size being too small, which the model might be improved if more images is supplied for training the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup an Image Generator\n",
    "---\n",
    "In order to try and mitigate problems of training a complex model to a small dataset, the dataset can be presented to the model in different ways to avoid the model over-training on wrong features within the dataset.\n",
    "\n",
    "This is achieved using a generator function which randomly performs transforms on the input dataset as it's fed to the model for training.\n",
    "\n",
    "We want this image generator to shift the input images in width and height in the range: 0.1, as well as introducing random shears in the range:0.1 and rotations of 15degree.\n",
    "\n",
    "We also want to perform horizontal flips of images randomly to allow our model to train better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenting training set images\n",
    "datagen = ImageDataGenerator(zoom_range=0,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1,     # Not sure if range is [0, 1] or 0.1\n",
    "                             rotation_range=15,\n",
    "                             horizontal_flip = True, vertical_flip = False)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a new model using a Generator\n",
    "---\n",
    "\n",
    "As above, but this-time we want to train using an image generator rather than the raw dataset.\n",
    "\n",
    "This means that random images are passed to our model during training. These images are based on the input dataset, but have been modified to help the model produce a more generic description of the input data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = create_cnn_model()\n",
    "model_aug.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model_aug.fit(datagen.flow(X_train, y_train, batch_size = BATCH_SIZE), \n",
    "                                        steps_per_epoch = X_train.shape[0] / BATCH_SIZE,\n",
    "                                        epochs = nb_epochs, verbose = 1,\n",
    "                                        validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again Look at the loss functions from this fit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Was this training with the generator better than training the same model on raw data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, as seen from the loss functions from this fit, the loss function for BOTH the training and validation dataset appears to be similar throughout the entire training process. This means that the model is no longer overtrained like it was on raw data by using the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What advantages are there to training using an image generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an image generator, one of the biggest advantage is the resolution of overtraining with the model appearing in the training with raw data, as with an image generator, the number of images are boosted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What disadvantages are there to using an image generator when training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generators applied onto the raw data can potentially alter the images, to the point which they no longer a correct representation of the dataset as they are meant to be. An simple example can be used to demonstrated this, with a 180 $\\degree$ rotate on images in the MNIST dataset, images in the classification of \"6\" would look very alike to images in the classification of \"9\", resulting in the model mis-classifying the images in class \"6\" and \"9\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE example with mnist dataset\n",
    "---\n",
    "\n",
    "This section goes over what is required to build a full VAE model to train over the mnist dataset.\n",
    "\n",
    "This is an example of performing an un-supervised training on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL1mDkVVjTAx"
   },
   "source": [
    "### Data preprocessing and cleaning:\n",
    "---\n",
    "\n",
    "As per the last workshop, load the mnist dataset and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evxspQgzhQA2"
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = (28, 28)\n",
    "\n",
    "# Load MNIST dataset-\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize dataset\n",
    "---\n",
    "As per last week we want to normalize our dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQckXNy4hfOn"
   },
   "outputs": [],
   "source": [
    "min_pixel_value = X_train.min().astype('float32')\n",
    "max_pixel_value = X_train.max().astype('float32')\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize the training and testing datasets-\n",
    "X_train /= max_pixel_value\n",
    "X_test /= max_pixel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhlx2T_uhnUT"
   },
   "outputs": [],
   "source": [
    "# convert class vectors/target to binary class matrices or one-hot encoded values-\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQWFT9oyhtOC"
   },
   "source": [
    "## Define Autoencoder using _Functional API_ & _Convolutional_ layers\n",
    "---\n",
    "\n",
    "Instead of using the sequential API for tensorflow, we now want to use the functional API:\n",
    "```\n",
    "eg:\n",
    "\n",
    "before we had:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SomeLayer)\n",
    "model.add(SomeLayer2)\n",
    "...\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "this now becomes:\n",
    "\n",
    "input = Input()\n",
    "x=SomeLayer()(input)\n",
    "x=SomeLayer2()(x)\n",
    "...\n",
    "model=Activation('softmax')(x)\n",
    "```\n",
    "\n",
    "Using this API you now need to finish the encoder and deoder model descriptions.\n",
    "\n",
    "\n",
    "In the encoder, ensure that every Conv2D layer is followed by a `gelu` activator.\n",
    "\n",
    "In the decoder ensure that the Conv2DTranspose layer uses (3, 3) kernels with 64, 64, 32 and 1 filters in total for each sequential layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our latent space\n",
    "---\n",
    "\n",
    "We are going to chose to use a latent space of 3 dimensions.\n",
    "The numer and size of the dimensions depends on the trade-off between the compression and size of the model and accuracy of the final output from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YME8yQznhzkW"
   },
   "outputs": [],
   "source": [
    "# Specify latent space dimensions-\n",
    "latent_space_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gfr__HdMh3cz"
   },
   "source": [
    "The last conv layer is flattened and connected to a Dense layer of size 2, which represents our 2-D latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLiym-Erh5N3"
   },
   "outputs": [],
   "source": [
    "# Define encoder-\n",
    "\n",
    "def get_encoder(encoder_output_dim):\n",
    "    encoder_input = Input(shape = (28,28,1))\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=3, strides = 2, padding = 'same')(encoder_input)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides = 2, padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides = 1, padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides = 1, padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = encoder_output_dim)(x)\n",
    "    encoder_output = x\n",
    "\n",
    "    encoder_model = Model(encoder_input, encoder_output)\n",
    "\n",
    "    return encoder_model, encoder_input, encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aX3S_3nnh-V4",
    "outputId": "a9bcaed8-5916-46f9-b3d5-5f8d182f64f3"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "encoder_model, encoder_input, encoder_output = get_encoder(latent_space_dim)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t41Rr_oLh6_P"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_decoder(decoder_space_dim):\n",
    "    decoder_input = Input(shape = decoder_space_dim)\n",
    "\n",
    "    x = Dense(units = 7 * 7 * 64)(decoder_input)\n",
    "    x = Reshape((7, 7, 64))(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=64, kernel_size=3, strides = (1, 1), padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=64, kernel_size=3, strides = (2, 2), padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=32, kernel_size=3, strides = (2, 2), padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=1, kernel_size=1, strides = (1, 1), padding = 'same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    decoder_output = x\n",
    "\n",
    "    decoder_model = Model(decoder_input, decoder_output)\n",
    "    \n",
    "    return decoder_model, decoder_input, decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrhkTZdGh1AA",
    "outputId": "d241db36-5b45-4407-af40-dfce60554ed7"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "decoder_model, decoder_input, decoder_output = get_decoder(latent_space_dim)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl4ANV-YimZ6"
   },
   "source": [
    "## Joining the Encoder to the Decoder\n",
    "To train the encoder and decoder simultaneously, we need to define a model that will represent the flow of an image through the encoder and back out through the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egJDvoUSh1KI"
   },
   "outputs": [],
   "source": [
    "# The complete autoencoder-\n",
    "\n",
    "# The input to the autoencoder is the same as the input to the encoder.\n",
    "vae_model_input = encoder_input\n",
    "\n",
    "# The output from the autoencoder is the output from the encoder passed through\n",
    "# the decoder.\n",
    "vae_model_output = decoder_model(encoder_output)\n",
    "\n",
    "# The Keras model that defines the full autoencoder—a model that takes an image,\n",
    "# and passes it through the encoder and back out through the decoder to generate\n",
    "# a reconstruction of the original image.\n",
    "vae_model = Model(vae_model_input, vae_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0dhzdrrix1j",
    "outputId": "e6e566ed-83ab-4f7f-a4a1-4c3798deb293"
   },
   "outputs": [],
   "source": [
    "# Final sanity check-\n",
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, make a prediction using the vae_model to see what the output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6T2UnzxizDc",
    "outputId": "d51b3f57-e205-4a44-da2e-377e510bc14a"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "vae_model(X_train[:1, :]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the output shape of the VAE model? Where do the dimension sizes come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the VAE model is used as a generative model for generating images that are similar to the MNIST dataset, the output shape of the VAE model should be the exact same as the input shape. And since we are using the MNIST dataset as input, with the dimensionality of (1, 28, 28), this is also the output shape of the VAE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using our Encoder\n",
    "---\n",
    "Like when we used the predict function with the DNN model using the Sequential API, we can now make 'predictions' using our encoder of the distribution of datapoints in our un-trained latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoded latent space representation for train images-\n",
    "encoded_X_train = encoder_model(X_train[:10000])\n",
    "encoded_X_train = encoded_X_train.numpy()\n",
    "print(\"encoded_X_train.shape: {}\".format(encoded_X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Projecting the latent space in 2D\n",
    "---\n",
    "\n",
    "Project the values in the latent space corresponding to the first 10,000 of the points in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent space of training images-\n",
    "plt.figure(figsize = (9, 7))\n",
    "y_labels=np.argmax(y_train, axis = 1)\n",
    "x = []\n",
    "y = []\n",
    "for i in range(10):\n",
    "    x = []\n",
    "    y = []\n",
    "    mask = y_labels == i\n",
    "    for j in encoded_X_train[mask]:\n",
    "        proj_lat_pt = j[:2]\n",
    "        x.append(proj_lat_pt[0])\n",
    "        y.append(proj_lat_pt[1])\n",
    "    plt.scatter(x, y, s=0.7, label='Label: %s' % j)\n",
    "plt.xlabel(\"latent space dim - 1\")\n",
    "plt.ylabel(\"latent space dim - 2\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST: Training images - Latent space 2D Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model features\n",
    "\n",
    "We will be using a custom loss function to give a spread distribution in our latent space.\n",
    "\n",
    "We also want to make use of an early_stopping funtion to make sure our model doesn't significatly over-train.\n",
    "\n",
    "In the case of a VAE the early_stopping function is easier to encode. It may be worth considering **why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAQCoaTBirUW"
   },
   "outputs": [],
   "source": [
    "def RMSE_loss(y_true, y_pred):\n",
    "    # RMSE loss function.\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1, 2, 3])\n",
    "\n",
    "# Compile defined autoencoder model-\n",
    "vae_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0003), loss = RMSE_loss, metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criterion-\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0.0001, patience = 3, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our VAE model\n",
    "---\n",
    "\n",
    "As before, now we want to train our VAE model.\n",
    "\n",
    "However, this is an un-supervised training as we will **not** be passing the labels of our dataset to the model and the intention is not to categorize our data, but to construct a decoder which can build images from our dataset based on their latent-space encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7csotk5-hx3p",
    "outputId": "6dd9f09e-ea5f-497a-b73d-15b33bb5bb8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train autoencoder-\n",
    "vae_training_hist = vae_model.fit(x = X_train, y = X_train,\n",
    "    batch_size = batch_size, shuffle = True, validation_data=(X_test, X_test), verbose=2, epochs = num_epochs, callbacks = [early_stopping, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the training output\n",
    "---\n",
    "\n",
    "Here again we want to look at the loss from the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize autoencoder training-\n",
    "\n",
    "plt.figure(figsize = (9, 7))\n",
    "\n",
    "## FINISH ME ##\n",
    "\n",
    "plt.title(\"Model loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlPnEo387CY2"
   },
   "source": [
    "## Analysis of _trained_ Autoencoder:\n",
    "---\n",
    "\n",
    "Again we can now make predictions using our encoder of our VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN5fvpEmjXmY",
    "outputId": "3f6a1c9d-4132-49e9-b0d8-f1f4c6e289fe"
   },
   "outputs": [],
   "source": [
    "# Get encoded latent space representation for train images-\n",
    "encoded_X_train = encoder_model(X_train[:10000])\n",
    "encoded_X_train = encoded_X_train.numpy()\n",
    "print(f\"encoded_X_train.shape: {encoded_X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tK0w9oq6jTA-",
    "outputId": "7e95546a-ae61-4e6b-a32b-5efe106f7a80"
   },
   "outputs": [],
   "source": [
    "# Get encoded latent space representations for test images-\n",
    "encoded_X_test = encoder_model(X_test)\n",
    "encoded_X_test = encoded_X_test.numpy()\n",
    "print(f\"encoded_X_test.shape: {encoded_X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Plot the distribution of images in 2D of our latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "mXjI2u3akC-L",
    "outputId": "f043fa5c-ad60-46bc-cedc-ad79394c796a"
   },
   "outputs": [],
   "source": [
    "# Visualize latent space of training images-\n",
    "plt.figure(figsize = (9, 7))\n",
    "y_labels=np.argmax(y_train, axis = 1)\n",
    "for i in range(10):\n",
    "\n",
    "    ## FINISH ME ##\n",
    "    \n",
    "plt.xlabel(\"latent space dim - 1\")\n",
    "plt.ylabel(\"latent space dim - 2\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST: Training images - Latent space 2D Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "rfFk48PUkq_Q",
    "outputId": "377aab84-9c4b-4486-8371-fbccfab0b925",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize latent space of validation images-\n",
    "plt.figure(figsize = (9, 7))\n",
    "y_labels=np.argmax(y_test, axis = 1)\n",
    "for i in range(10):\n",
    "\n",
    "    ## FINISH ME ##\n",
    "    \n",
    "plt.xlabel(\"latent space dim - 1\")\n",
    "plt.ylabel(\"latent space dim - 2\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST: Validation images -Latent space 2D Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDau_giM6Gi-"
   },
   "source": [
    "## Visualize Actual vs. Recreated MNIST Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDau_giM6Gi-"
   },
   "source": [
    "### Q: Using trained autoencoder show 5 're-generated' images from our VAE model and compare this to their original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "UOmsaCsF4hyq",
    "outputId": "dee33f4c-26de-40b3-8d39-c3a655d3c81d"
   },
   "outputs": [],
   "source": [
    "recreated_image = vae_model(X_train[:1, :])\n",
    "recreated_image = recreated_image.numpy().reshape(28, 28)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(X_train[:1, :].reshape(28, 28), cmap = 'gray')\n",
    "axarr[1].imshow(recreated_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "5svzUFPT54UF",
    "outputId": "f9d108c1-7096-4b3f-96fc-a9712ca62028"
   },
   "outputs": [],
   "source": [
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "11zUUBkI6SdR",
    "outputId": "f46bd30c-1bba-4121-e665-690977579cf4"
   },
   "outputs": [],
   "source": [
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "N88zKD2W6gBn",
    "outputId": "e1b3efc4-5f83-486c-a980-dca2d768d3f1"
   },
   "outputs": [],
   "source": [
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "2PNyuN-NrH0H",
    "outputId": "f3daec6e-d3a1-4528-f4fa-6e7c465b9254"
   },
   "outputs": [],
   "source": [
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WBUWE65rLNl"
   },
   "source": [
    "# Anomaly detection\n",
    "---\n",
    "\n",
    "We have introduced auto-encoders as a form of unsupervised learning, since we are not using the image labels during training. This means that auto-encoders are not ideal for image classification (at least not since we actually _have_ the labels), but they can be used for something else: anomaly detection. This is the task of identifying examples that the model considers \"anomalous\" with respect to the dataset used during training. \n",
    "\n",
    "First, we'll load in some \"anomalous\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(shuffle=True)['images']\n",
    "faces = faces[:,4:-4:2,4:-4:2,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the face images and the auto-encoder result\n",
    "\n",
    "* How many samples does the \"outlier\" dataset (_i.e._ `faces`) contain?\n",
    "* What is the shape of the images and what is the range of pixel intensities? Does this conform with the preprocessed MNIST images?\n",
    "* Display the first few face images.\n",
    "* Get the output/prediction of the auto-encoder from the previous section when applied to all of the faces.\n",
    "* Show the auto-encoded versions of the same faces you showed above. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## FINISH ME ##\n",
    "## (first look at the dataset again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Indicative answers:_\n",
    "    \n",
    "* The face images have the same shape and pixel intensity range as the preprocessed MNIST images, so they are valid inputs to the auto-encoder.\n",
    "* We can see that the auto-encoder transformed faces are _very_ unlike the input images. This is because the auto-encoder was trained to learn an efficient representation of hand-written digits which is not necessarily an efficient representation for other image domains, _e.g._ faces. This examples shows that this is clearly the case. From the point of view of the auto-encoder, images of faces are _anomalies_ in that they are fundamentally unlike the images on which it was trained, and therefore we shouldn't expect it to do a good job in encoding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform anomaly detection\n",
    "Imagine now that we had a dataset comprised of mostly MNIST images, but also a small subset of anomalies or \"outliers\"; here, in the form of black-and-white images of facses, with the same shape as the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined MNIST images and \"outliers\" in a mixed dataset\n",
    "mixed = np.vstack((X_test, faces))\n",
    "\n",
    "# Shuffle the mixed dataset so the \"outliers\" are randomly distributed\n",
    "indices = np.random.permutation(mixed.shape[0])\n",
    "mixed   = mixed[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto-encoder was trained to minimise the difference between the original and the auto-encoded image, so let's use binary cross-entropy (BCE) as our metric for the difference between an image and its auto-encoded version. The `binary_crossentropy` method provided below computes **pixel-wise BCE** for two (arrays of) images: the input and the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy (img_in, img_out):\n",
    "    assert img_in.shape == img_out.shape\n",
    "    eps = np.finfo(float).eps\n",
    "    img_out = np.clip(img_out, eps, 1. - eps)\n",
    "    return - (img_in * np.log(img_out) + (1 - img_in) * np.log(1 - img_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Anomaly Detection\n",
    "---\n",
    "* Define a new method called `difference` which takes as input two arrays of images (`img_in` and `img_out`; similar to the `binary_crossentropy`); computes the **average BCE value for each image,** or row; and returns a vector of these difference measures.\n",
    "* For each image, or row, in `mixed`, compute the difference score of the auto-encoded image wrt. the original image. This is a measure of how \"inlier\"- or \"outlier\"-like an image is.\n",
    "* Make a histogram of these scores, and see if you can identify any structure. It might be useful to use a logarithmic x-axis (see `plt.xscale`) along with logarithmic x-axis bins (see `np.logspace`).\n",
    "* Show the 9 least and the 9 most outlying images, according to this difference score. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference (img_in, img_out):\n",
    "    ## This method effectively calculates the difference between what is expected and what was prediced\n",
    "    return ## FINISH ME ##\n",
    "\n",
    "## Use the pre-trained mnist model to make predictions on our dataset\n",
    "p_mixed = vae_model.predict(mixed)\n",
    "score   = difference(mixed, p_mixed)\n",
    "\n",
    "## Plot the distribution of the magnitude of the bce values for different images\n",
    "\n",
    "## FINISH ME ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices_scores = sorted(zip(np.arange(score.size), score), key=lambda p: p[1])\n",
    "\n",
    "best_indices  = list(list(zip(*sorted_indices_scores[  :9]))[0])\n",
    "worst_indices = list(list(zip(*sorted_indices_scores[-9: ]))[0])\n",
    "\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(mixed[best_indices].reshape(9*28,28));\n",
    "axarr[1].imshow(mixed[worst_indices].reshape(9*28,28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` ## FINISH ME ## ```  Discuss what images are at the 2 extremes of the latent-space when reduced to 1D, i.e. which are best reconstructed and maybe why and which images aren't and why that may be the case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoEncoder_MNIST_TF2_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
