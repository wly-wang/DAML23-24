{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================#\n",
    "# Import relevant packages #\n",
    "#==========================#\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad\n",
    "from scipy.special import erfinv\n",
    "from iminuit import Minuit\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import binom\n",
    "%matplotlib inline\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report 1 - Estimating Parameters with MC Generated Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The report based around a particle decay of $X \\rightarrow D$, and a parameter related to the matter/anti-matter asymmetry of the Universe will be measured.\n",
    "\n",
    "The PDF of the relevant particle decay is described by:\n",
    "$$\n",
    "P(t;\\tau,\\Delta m_s, V) \\propto (1+V\\sin{(\\Delta mt)}) \\times \\exp{-\\frac{t}{\\tau}}\n",
    "$$\n",
    "where\n",
    "\n",
    "* $t$ is the observable quantity - the decay time of each decay.\n",
    "* $\\tau$ is a lifetime parameter.\n",
    "* $\\Delta m$ is a mass difference parameter which leads to sinusoidal oscillations superimposed on the exponential decay.\n",
    "* $V$ is a parameter which measures matter/anti-matter asymmetry and has the value zero if\n",
    "the universe is symmetric.\n",
    "\n",
    "And the nominal value of the parameters are:\n",
    "\n",
    "* $\\tau = 1.5$\n",
    "* $\\Delta m = 20.0$\n",
    "* $V = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first plot the PDF to see the distribution within the range 0 to 10. At this stage, something else in addition is to vary the parameters a little to see how change of each parameter affects our PDF distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================#\n",
    "# Create a function returning the PDF distribution #\n",
    "#==================================================#\n",
    "\n",
    "def pdf_distribution(t, tau, delta_m, V):\n",
    "    return ( (1 + V*np.sin(delta_m*t))*np.exp(-t/tau) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================#\n",
    "# Define parameters for plotting the PDF distribution #\n",
    "#=====================================================#\n",
    "\n",
    "loBound = 0.\n",
    "hiBound = 10.\n",
    "n_interval = 1000\n",
    "t = np.linspace(loBound, hiBound, n_interval, endpoint=True)\n",
    "# nominal values for the parameters\n",
    "tau1 = 1.5\n",
    "delta_m1 = 20.\n",
    "v1 = 0.1\n",
    "# Three more sets of parameters to see how our PDF distribution change\n",
    "tau2 = 3.\n",
    "delta_m2 = 10.\n",
    "v2 = 1.\n",
    "\n",
    "tau3 = 2.\n",
    "delta_m3 = 10.\n",
    "v3 = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================#\n",
    "# Plot the PDF distribution #\n",
    "#===========================#\n",
    "\n",
    "plt.plot(t, pdf_distribution(t, tau1, delta_m1, v1), ls='-.', color='r', label='Nominal Parameters Values')\n",
    "plt.plot(t, pdf_distribution(t, tau2, delta_m2, v2), ls='--', color='g', label='Set 1 Varied Parameters Values')\n",
    "plt.plot(t, pdf_distribution(t, tau3, delta_m3, v3), color='orange', label='Set 2 Varied Parameters Values')\n",
    "plt.title(r'Probability Density Function for the Particle Decay of $X \\rightarrow D$')\n",
    "plt.xlabel('Time (A.U.)')\n",
    "plt.ylabel(r'$P(t;\\tau,\\Delta m, V)$ (A.U.)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the distribution of our PDF that characterizes the particle decay, and three of them have different parameters, this is to show how the parameters change the waveform of our PDF distribution.\n",
    "\n",
    "As seen from above, and equation from the top description section. It appears that the function is decaying in a sinusoidal way. And the parameters ($\\tau, \\Delta m, V$) represents  the decay constant, the angular frequency and the amplitude, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "In this question, we'll use toy Monte Carlo event generation to simulate multiple pseudo-experiments, and use this method to determine the expected statistical precision with which one could measure each of the parameters with 10000 events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step here is to create a PDF class which generates random points based on the PDF function given in the description for the relevant particle decay, and also contain methods/functions in setting different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================#\n",
    "# Create a PDF class used to model the oscillatory exponential decay pdf function as described #\n",
    "#==============================================================================================#\n",
    "\n",
    "class DecayPDF(object):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, loBound, hiBound, tau, delta_m, V):\n",
    "        self.loBound = loBound\n",
    "        self.hiBound = hiBound\n",
    "        self.tau = tau\n",
    "        self.delta_m = delta_m\n",
    "        self.V = V\n",
    "    \n",
    "        # initialize distribution mass array\n",
    "        self.mass = []\n",
    "\n",
    "    \n",
    "    #———————————————————————————————————————————#\n",
    "    # Function used to find maximum of function #\n",
    "    #———————————————————————————————————————————#\n",
    "\n",
    "    def find_max(self):\n",
    "        # First generate a grid of x points\n",
    "        x = np.linspace(self.loBound, self.hiBound, endpoint=True, num=100000)\n",
    "        # Next, evaluate the function at all the x, and return the maximum\n",
    "        y = self.evaluate(x)\n",
    "        return y.max()\n",
    "\n",
    "    \n",
    "    #———————————————————————————————————————————#\n",
    "    # Function used to find minimum of function #\n",
    "    #———————————————————————————————————————————#\n",
    "\n",
    "    def find_min(self):\n",
    "        # First generate a grid of x points\n",
    "        x = np.linspace(self.loBound, self.hiBound, endpoint=True, num=100000)\n",
    "        # Next, evaluate the function at all the x, and return the minimum\n",
    "        y = self.evaluate(x)\n",
    "        return y.min()\n",
    "\n",
    "\n",
    "    #————————————————————————————————————————————————#\n",
    "    # Function used to set passed vars as parameters #\n",
    "    #————————————————————————————————————————————————#\n",
    "\n",
    "    def setParameters(self, tau, delta_m, V):\n",
    "        self.tau = tau\n",
    "        self.delta_m = delta_m\n",
    "        self.V = V\n",
    "\n",
    "\n",
    "    #————————————————————————————————————————————————#\n",
    "    # Evaluate the PDF and return un-normalized vals #\n",
    "    #————————————————————————————————————————————————#\n",
    "\n",
    "    def evaluate(self, t):\n",
    "        return ( (1+self.V*np.sin(self.delta_m*t))*np.exp(-t/self.tau) )\n",
    "\n",
    "    \n",
    "    #—————————————————————————————————————————————————————————————————————————————————————————————#\n",
    "    # From the PDF distribution for our decay, sample random points (here using the 'box' method, #\n",
    "    # which this technique uses the generation of two random numbers, starting from anuniform     #\n",
    "    # random number generator. Consider the algorithm equivalent to generating random numbers     #\n",
    "    # in a “box”. Then only keeping those which fall under the curve.                             #\n",
    "    #—————————————————————————————————————————————————————————————————————————————————————————————#\n",
    "\n",
    "    def next(self):\n",
    "        doLoop = True\n",
    "        while (doLoop):\n",
    "            x = np.random.uniform(self.loBound, self.hiBound)\n",
    "            y1 = self.evaluate(x)\n",
    "            y2 = np.random.uniform(self.find_min(), self.find_max())\n",
    "            # now consider the acceptance if y1 is above x-axis\n",
    "            if (y2<y1):\n",
    "                filtered_x = x\n",
    "                self.mass.append(x)\n",
    "                return filtered_x\n",
    "\n",
    "\n",
    "    #———————————————————————————————————————————#\n",
    "    # Function which returns the mass attribute #\n",
    "    #———————————————————————————————————————————#\n",
    "\n",
    "    def returnMass(self):\n",
    "        return np.array(self.mass)\n",
    "\n",
    "\n",
    "    #——————————————————————————————————————#\n",
    "    # Function which clears the mass array #\n",
    "    #——————————————————————————————————————#\n",
    "\n",
    "    def flushMass(self):\n",
    "        self.mass.clear()\n",
    "\n",
    "\n",
    "    #————————————————————————————————————————————————#\n",
    "    # Evaluate the integral of the pdf within limits #\n",
    "    #————————————————————————————————————————————————#\n",
    "\n",
    "    def integrate(self, loBound, highBound):\n",
    "        integral_result, integral_err = quad(self.evaluate, loBound, highBound)\n",
    "        return integral_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's first generate a toy dataset with the PDF class we wrote in the cell above, and make a histogram for this dataset to test that our PDF class is running and working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal parameter values\n",
    "tau_nom = 1.5\n",
    "delta_m_nom = 20.\n",
    "V_nom = 0.1\n",
    "\n",
    "# Initialize the PDF class and feed in the nominal parameters\n",
    "pdf = DecayPDF(loBound, hiBound, tau_nom, delta_m_nom, V_nom)\n",
    "\n",
    "# Generate 10000 events for this first test toy dataset\n",
    "nEvents = 10000\n",
    "for i in range(nEvents):\n",
    "    pdf.next()\n",
    "\n",
    "# Return the mass array and flush them so that it resets and the class can be used again for generating new toy datasets\n",
    "evts = pdf.returnMass()\n",
    "pdf.flushMass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our first generated dataset with our pdf class\n",
    "plt.hist(evts, 150, color='orange', label='Generated Data')\n",
    "plt.title('Histogram for the First Generated Dataset Using the PDF Class')\n",
    "plt.xlabel('Time (A.U.)')\n",
    "plt.ylabel('Event Entries')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above, our first generated dataset appears to be sensible, following the general trend of the pdf function as we expect. Next, we would like to use a maximum likelihood fit to determine the precision expected for each of the parameters $\\tau$,$\\Delta m$ and $V$. In order to do this, 100 generated toy datasets need to be fitted.\n",
    "\n",
    "First, we will write a maximum likelihood fit class for this, using the negative of the log of the joint likelihood. Then we will fit using the class below on the 1 generated dataset above to see whether our class works well or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================================#\n",
    "# Create class for negative log-likelihood minimization statistic used for fitting the pdf #\n",
    "#==========================================================================================#\n",
    "\n",
    "class NegativeLLcalculator(object):\n",
    "    \n",
    "    def __init__(self, pdf, data):\n",
    "        self.pdf = pdf\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    # function used to update data, mainly for the ability to recycle this class for different fit pdfs\n",
    "    def updateData(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    # Function used to calculate log likelihood\n",
    "    def calc_neg_LL(self):\n",
    "        likelihood = self.pdf.evaluate(self.data)/(self.pdf.integrate(self.pdf.loBound, self.pdf.hiBound))\n",
    "        log_likelihood = np.log(likelihood)\n",
    "        return -log_likelihood.sum()\n",
    "\n",
    "    # To calcualte an NLL for our pdf\n",
    "    def evaluate(self, tau, delta_m, V):\n",
    "        nll = 0.\n",
    "        self.pdf.setParameters(tau, delta_m, V)\n",
    "        # here, compute the log likelyhood\n",
    "        return self.calc_neg_LL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use iMinuit to fit our dataset with the minimization statistics written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fit parameters in a dictionary\n",
    "init_params = {\n",
    "        'tau':                  1.5,\n",
    "        'delta_m':              20.,\n",
    "        'V':                    0.1,\n",
    "}\n",
    "\n",
    "# First initialize the pdf classes for function we use to fit, and the fitting statistics\n",
    "pdf = DecayPDF(loBound, hiBound, **init_params)\n",
    "fit_stats = NegativeLLcalculator(pdf, evts)\n",
    "\n",
    "# Initialize iminuit minimizer\n",
    "minimizer = Minuit(fit_stats.evaluate, **init_params)\n",
    "\n",
    "# Set the error difference to 0.5 (we are using NegativeLL)\n",
    "minimizer.errordef = 0.5\n",
    "\n",
    "# Fit for best parameters\n",
    "mresult = minimizer.migrad()\n",
    "mresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the Minuit minimizer is shown, with the fitted parameters for this 1 dataset as following:\n",
    "* $\\tau = 1.453 \\pm 0.015$ (A.U.)\n",
    "* $\\Delta m = 19.93 \\pm 0.08$ (A.U.)\n",
    "* $ V = 0.099 \\pm 0.014$ (A.U.)\n",
    "\n",
    "Now, with the idea that we can have somewhat of a good fit on the parameters, now we would like to determine the expected precision on these fitted parameters. This is gonna be done by generating 100 toy datasets, each containing 10000 events like the dataset we generated above, and we will fit each one of them, store the fitted values and then obtain the precision and show the results.\n",
    "\n",
    "First thing here is to start by creating a function which will be used to generate and fit datasets, and then store these fitted parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fit_data(loBound, hiBound, nominal_params):\n",
    "    # define a dictionary containing initial guess of params\n",
    "    initial_params = {\n",
    "        'tau'      : 1.1,\n",
    "        'delta_m'  : 20.4,\n",
    "        'V'        : 0.11,\n",
    "    }\n",
    "\n",
    "    # initialize pdf \n",
    "    pdf = DecayPDF(loBound, hiBound, *nominal_params.values())\n",
    "\n",
    "    # Generate event in a loop (same as before)\n",
    "    for i in range(nEvents):\n",
    "        pdf.next()\n",
    "    \n",
    "    # Return the mass array and flush them so that it resets and the class can be used again for generating new toy datasets\n",
    "    evts = pdf.returnMass()\n",
    "    pdf.flushMass()\n",
    "\n",
    "    # Now initialize the maximum likelyhood minimizing statistics\n",
    "    fit_statistics = NegativeLLcalculator(pdf, evts)\n",
    "    minuit_minimizer = Minuit(fit_statistics.evaluate, **initial_params)\n",
    "    results = minuit_minimizer.migrad()\n",
    "\n",
    "    # Now store the parameter values\n",
    "    params_vals = []\n",
    "    for i in initial_params.keys():\n",
    "        params_vals.append(results.params[i].value)\n",
    "    return params_vals    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the above function, let's loop over 100 times to generate 100 toy dataset and fit them, each with 10000 events, and obtaining their optimized parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters again for completeness purpose\n",
    "nominal_params_vals = {\n",
    "    'tau'           : 1.5,\n",
    "    'delta_m'       : 20.,\n",
    "    'V'             : 0.1,\n",
    "}\n",
    "\n",
    "# Define a dictionary with empty lists for holding optimized params values\n",
    "optimized_params_val_dict = {\n",
    "    'tau_optim'           : [],\n",
    "    'delta_m_optim'       : [],\n",
    "    'V_optim'             : [],\n",
    "}\n",
    "\n",
    "# Now generate and fit 100 toy datasets\n",
    "n_toy_datasets = 100\n",
    "for i in range(n_toy_datasets):\n",
    "    # Use our function for generating and fitting dataset\n",
    "    optim_params_vals = generate_fit_data(loBound, hiBound, nominal_params_vals)\n",
    "\n",
    "    # Then append the returned optimized parameter values to our dictionary\n",
    "    for idx, key in enumerate(optimized_params_val_dict):\n",
    "        optimized_params_val_dict[key].append(optim_params_vals[idx])\n",
    "\n",
    "# Now our dict should contain the optimized parameters, now convert each list into np.array\n",
    "for key in optimized_params_val_dict:\n",
    "    optimized_params_val_dict[key] = np.array(optimized_params_val_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our optimized parameters from 100 toy datasets, the precision of our parameters can be calculated by taking the mean values of each list in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision of our parameters\n",
    "tau_precision = optimized_params_val_dict['tau_optim'].mean()\n",
    "delta_m_precision = optimized_params_val_dict['delta_m_optim'].mean()\n",
    "V_precision = optimized_params_val_dict['V'].mean()\n",
    "\n",
    "print('The tau parameter precision for our MC generated simulation is %.3f (A.U.).' % tau_precision)\n",
    "print('The delta m parameter precision for our MC generated simulation is %.3f (A.U.).' % tau_precision)\n",
    "print('The V parameter precision for our MC generated simulation is %.3f (A.U.).' % tau_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the precision of our parameters explored and calculated, we would like to take a look at the bias for our parameters. The way we look at the bias here is by defining it to be the difference between the optimized parameter precision and the nominal truth parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the bias on the parameters and print them\n",
    "tau_bias = abs(nominal_params_vals['tau'] - tau_precision)\n",
    "tau_bias = abs(nominal_params_vals['delta_m'] - delta_m_precision)\n",
    "tau_bias = abs(nominal_params_vals['V'] - V_precision)\n",
    "\n",
    "print('The tau parameter bias for our MC generated simulation is %.3f (A.U.).' % tau_bias)\n",
    "print('The delta m parameter bias for our MC generated simulation is %.3f (A.U.).' % delta_m_bias)\n",
    "print('The V parameter bias for our MC generated simulation is %.3f (A.U.).' % V_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to calculate and assess the bias on the bias we calculated above for our optimized precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
